{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoevYWAsNAQQ"
      },
      "source": [
        "#Labo 4 : Classifieurs Linéaires et Descente de Gradient\n",
        "# 15 octobre 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccM0p87gx4gS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# on définit un noyau pour rendre nos résultats déterministes\n",
        "# (pour que tout le monde ait les mêmes resultats.)\n",
        "np.random.seed(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dTaFfwh3gHE"
      },
      "source": [
        "# Fonctions de perte et gradients\n",
        "\n",
        "Nous considérons ici des données labellisées $(x,y)$, $x$ est une entrée de dimension $d$ et $y\\in\\{-1,1\\}$. Notre dataset contient n de ces paires. L'objectif de cette séance sera d'entraîner des classifieurs linéaires sur ce dataset. Ces classifieurs linéaires font la prédiction:\n",
        "$$f( x ; w, b) = \\text{sign}(w^T x + b) = 1 \\quad \\text{si}\\quad w^T x + b\\geq 0 \\quad\\text{et}\\quad -1 \\quad\\text{sinon.}$$\n",
        "\n",
        "Afin de simplifier le code et la notation, nous pouvons nous débarasser du terme du biais $b$. Pour ce faire, nous concatènons un $1$ à chaque vecteur $x$ de façon à ce que $x' = (x, 1) \\in \\mathbb{R}^{d+1}$ puis nous concatènons $b$ à $w$ de sorte que $w' = (w, b)$. Ainsi, $w^T x + b = w'^T x'$ et nous pouvons tout écrire en terme de transformations linéaires à la place de transformations affines. Nous pouvons maintenant omettre le terme de biais.\n",
        "\n",
        "L'erreur d'entraînement est définie comme étant la moyenne des erreurs obtenues dans le dataset:\n",
        "\n",
        "$$\\text{Error}(w) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\{y_i \\neq f(x_i ; w) \\}$$\n",
        "\n",
        "où $\\mathbb{1}\\{A\\}$ est la fonction indicatrice.\n",
        "\n",
        "**Exercice:** Nous ne pouvons pas directement minimiser cette fonction. *Pourquoi ?*\n",
        "\n",
        "Puisque nous ne pouvons pas minimiser cette expression, nous allons plutôt minimiser 4 différentes fonctions de perte qui sont typiquement utilisées avec des modèles linéaires. Ces fonctions sont appelées des fonctions convexes de substitution. Elles sont convexes et possède un (sous-)gradient donc nous pouvons les optimiser avec la descente du gradient.\n",
        "\n",
        "* régression linéaire $g(w; x, y) = \\frac{1}{2}(x^T w - y)^2$.\n",
        "* perceptron $g(w; x, y) = \\max(0, -y x^T w)$.\n",
        "* SVM $g(w; x, y) = \\max(0, 1 - y x^T w)$.\n",
        "* régression logistique $g(w; x, y) = \\log( 1 + e^{- y x^T w})$.\n",
        "\n",
        "La fonction objectif que nous allons minimiser est la moyenne des pertes de chaque exemple additionné à une régularisation de type $\\ell^2$ d'hyperparamètre $\\lambda$:\n",
        "\n",
        "$$L(w) = \\frac{1}{n}\\sum_{i=1}^n g(w; x_i, y_i) + \\frac{\\lambda}{2} \\| w\\|^2 $$\n",
        "\n",
        "Notez que pour le SVM, le terme de régularisation sert en fait à trouver la marge maximale de la frontière de décision.\n",
        "\n",
        "**Exercice:** Complétez les fonctions suivantes qui effectuent le graphique de ces pertes en fonction de $z = x^T w$ en assumant que $y = 1$. Complétez également la fonction `error_counter` qui représente $\\mathbb{1}\\{y \\neq f(x ; w) \\} = \\mathbb{1}\\{ x^T w <0 \\} = \\mathbb{1}\\{ z <0 \\} $.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSah0vTsAxTn"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "\n",
        "def error_counter(z):\n",
        "    # écrivez votre code ici\n",
        "    return\n",
        "\n",
        "def linearloss(z):\n",
        "    # écrivez votre code ici\n",
        "    return\n",
        "\n",
        "def logisticloss(z):\n",
        "    # écrivez votre code ici\n",
        "    return\n",
        "\n",
        "def perceptronloss(z):\n",
        "    # écrivez votre code ici\n",
        "    return\n",
        "\n",
        "def svmloss(z):\n",
        "    # écrivez votre code ici\n",
        "    return\n",
        "\n",
        "zz = np.linspace(-3,3,1000)\n",
        "plt.figure()\n",
        "for loss in [error_counter, linearloss, logisticloss,perceptronloss, svmloss]:\n",
        "    plt.plot(zz, loss(zz), label=loss.__name__)\n",
        "\n",
        "plt.ylim(-.5,4.5)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzGlDb2FAwqh"
      },
      "source": [
        "**Exercice:** Dérivez le gradient par rapport à $w$ pour chacune des fonctions de pertes définies ci-dessus.\n",
        "\n",
        "**Exercice bonus:** Toutes ces fonctions de pertes ont un gradient qui est définit comme étant un scalaire multiplié pas $x$. Encore une fois, faites un graphique de ces scalaires en assumant que $y=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn5Clo1SgOYa"
      },
      "source": [
        "## 1. Prétraitement de données\n",
        "\n",
        "Pour notre problème de classification binaire, nous allons utiliser uniquement deux classes du dataset iris, soit la classe 1 et la classe 2. Ainsi, seulement les exemples appartenant à la classe 1 ou 2 seront considérés et par mesure de simplicité, nous allons changer leurs étiquettes à 1 et -1. Nous allons de plus utiliser seulement que deux caractéristiques (`feature`) des fleurs du dataset iris pour chaque exemple. Ceci permet de facilement visualiser les frontières de décisions.\n",
        "\n",
        "Voici le code pour le pré-traitement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut0kKy_pgJLJ"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    iris = np.loadtxt('http://www.iro.umontreal.ca/~dift3395/files/iris.txt')\n",
        "else:\n",
        "    iris = np.loadtxt('iris.txt')\n",
        "\n",
        "def preprocess(data, label_subset, feature_subset, n_train):\n",
        "\n",
        "    \"\"\"Effectue une partition aléatoire des données en sous-ensembles\n",
        "    train set et test set avec le sous-ensemble de classes label_subset\n",
        "    et le sous-ensemble de feature feature_subset\n",
        "    \"\"\"\n",
        "    # on extrait seulement les classes de label_subset\n",
        "    data = data[np.isin(data[:,-1],label_subset),:]\n",
        "\n",
        "\n",
        "    # on transforme les classes pour qu'elles soient [-1, 1]\n",
        "    if len(label_subset)!=2:\n",
        "        raise UserWarning('We are exclusively  dealing with binary classification.')\n",
        "    data[data[:,-1]==label_subset[0],-1] = -1\n",
        "    data[data[:,-1]==label_subset[1],-1] = 1\n",
        "\n",
        "    # on extrait les features et leurs étiquettes\n",
        "    data = data[:, feature_subset + [-1]]\n",
        "\n",
        "    # on ajoute une colonne pour le biais\n",
        "    data = np.insert(data, -1, 1, axis=1)\n",
        "\n",
        "    # on sépare en train et test\n",
        "    inds = np.arange(data.shape[0])\n",
        "    np.random.shuffle(inds)\n",
        "    train_inds = inds[:n_train]\n",
        "    test_inds = inds[n_train:]\n",
        "    trainset = data[train_inds]\n",
        "    testset = data[test_inds]\n",
        "\n",
        "    # on normalise les données pour qu'elles soient de moyenne 0\n",
        "    # et d'écart-type 1 par caractéristique et on applique\n",
        "    # ces mêmes transformations au test set\n",
        "    mu = trainset[:,:2].mean(axis=0)\n",
        "    sigma  = trainset[:,:2].std(axis=0)\n",
        "    trainset[:,:2] = (trainset[:,:2] -mu)/sigma\n",
        "    testset[:,:2] = (testset[:,:2] -mu)/sigma\n",
        "\n",
        "    return trainset, testset\n",
        "\n",
        "\n",
        "trainset, testset = preprocess(iris, label_subset=[1,2], feature_subset=[2,3], n_train=75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWHyKwpLflUG"
      },
      "source": [
        "## 2. Quelques fonctions utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qk6dvyFrYhD"
      },
      "outputs": [],
      "source": [
        "def scatter(theset, marker='o'):\n",
        "    d1 = theset[theset[:, -1] > 0]\n",
        "    d2 = theset[theset[:, -1] < 0]\n",
        "    plt.scatter(d1[:, 0], d1[:, 1], c='b', marker=marker, label='class 1', alpha=.7)\n",
        "    plt.scatter(d2[:, 0], d2[:, 1], c='g', marker=marker, label='class 0', alpha=.7)\n",
        "    plt.xlabel('x_0')\n",
        "    plt.ylabel('x_1')\n",
        "\n",
        "\n",
        "def finalize_plot(title):\n",
        "    plt.title(title)\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "\n",
        "scatter(trainset, marker='x')\n",
        "scatter(testset, marker='^')\n",
        "finalize_plot('train and test data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fngg_Cb4QIs"
      },
      "source": [
        "Nous représentons et définissons ci-dessous la frontière de décision des modèles linéaires que nous allons entraîner. Comme la règle de décision pour faire la prédiction d'un exemple est commune à tous ces modèles, la frontière de décision est alors calculée de la même façon, c'est-à-dire en isolant $x_1$ dans $f(x) = x^{T}w = 0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_mmiKraGRLV"
      },
      "outputs": [],
      "source": [
        "def decision_boundary(w):\n",
        "    if w[1]==0:\n",
        "        raise RuntimeWarning(\"This decision boundary is either vertical, either undefined.\")\n",
        "\n",
        "    # hack to avoid changing the boundaries\n",
        "    # astuce pour éviter de changer les limites\n",
        "    xlim = plt.xlim()\n",
        "    ylim = plt.ylim()\n",
        "\n",
        "    xx = np.linspace(-10, 10, 2)\n",
        "    yy = -(w[2] + w[0]*xx)/w[1]\n",
        "    plt.plot(xx, yy, c='r', lw=2, label='f(x)=0')\n",
        "\n",
        "    # hack to avoid changing the boundaries\n",
        "    # astuce pour éviter de changer les limites\n",
        "    plt.xlim(xlim)\n",
        "    plt.ylim(ylim)\n",
        "\n",
        "w0 = np.array([1,-1,1])\n",
        "scatter(trainset)\n",
        "decision_boundary(w0)\n",
        "finalize_plot('A random classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugazaI1JgW_j"
      },
      "source": [
        "## 3. Classe Parent\n",
        "\n",
        "Nous définissons ici une classe parent qui sera passée par la suite dans les classes enfants qui sont présentées ci-dessous. En appellant la fonction `super().__init__` à l'intérieur des classes enfants, nous pouvons ainsi hériter d'attributs de la classe parent sans devoir les implémenter à nouveau. Nous avons une classe enfant pour chacun des classifieurs linéaires et dans ces classes, votre rôle sera de complétez les méthodes `loss` et `gradient`.\n",
        "\n",
        "#### La Descente de Gradient\n",
        "Nous voulons minimiser la fonction de perte $L$. Cette fonction est différentiable, donc nous pouvons utiliser l'algorithme de descente de gradient: commencer par n'importe quelle initialisation du paramètre $w_0$ et répéter pour $t\\in\\{0, \\dots, t_\\max \\}$:\n",
        "$$w_{t+1} = w_t - \\eta \\nabla L (w_t) \\; .$$\n",
        "Sous certaines conditions sur la taille du pas $\\eta$ et de la perte $L$, cet algorithme est garanti de converger à un minimum de $L$. Vous devez compléter cet algorithme dans cette classe parent.\n",
        "\n",
        "#### Fonction de prédiction\n",
        "Vous devrez remplir la fonction `predict` de cette classe. Cette fonction doit retourner les prédictions étant donnée une matrice de données `X` (de taille $n_{exemples} \\times n_{features}$).\n",
        "\n",
        "\n",
        "#### Fonction de test\n",
        "Vous devrez remplir la fonction `error_rate` de cette classe. Cette fonction prend en entrée une matrice de données `X` (de taille $n_{exemples} \\times n_{features}$) et les étiquettes correspondantes (`y`, vecteur de taille $n_{examples}$), et retourne l'erreur moyenne du classifieur sur ces données-là.\n",
        "\n",
        "#### Fonction d'entraînement\n",
        "Vous devrez écrire le code de la boucle for de la fonction `train`. Chaque étape de cette boucle doit effectuer une mise-à-jour du paramètre $w$ avec la méthode du gradient, et mettre-à-jour les listes `losses` et `errors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XqTgs7nE7jL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class LinearModel:\n",
        "    \"\"\"\"Classe parent pour tous les modèles linéaires.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, w0, reg):\n",
        "        \"\"\"Les poids et les biais sont définis dans w.\n",
        "        L'hyperparamètre de régularisation est reg.\n",
        "        \"\"\"\n",
        "        self.w = np.array(w0, dtype=float)\n",
        "        self.reg = reg\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Retourne f(x) pour un batch X\n",
        "        \"\"\"\n",
        "        # écrivez votre code ici\n",
        "        return np.zeros(X.shape[0])\n",
        "\n",
        "    def error_rate(self, X, y):\n",
        "        \"\"\"Retourne le taux d'erreur pour un batch X\n",
        "        \"\"\"\n",
        "        # écrivez votre code ici\n",
        "        return 0.\n",
        "\n",
        "    # les méthodes loss et gradient seront redéfinies dans les classes enfants\n",
        "    def loss(self, X, y):\n",
        "        return 0\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        return self.w\n",
        "\n",
        "    def train(self, data, stepsize, n_steps, plot=False):\n",
        "        \"\"\"Faire la descente du gradient avec batch complet pour n_steps itération\n",
        "        et un taux d'apprentissage fixe. Retourne les tableaux de loss et de\n",
        "        taux d'erreur vu apres chaque iteration.\n",
        "        \"\"\"\n",
        "\n",
        "        X = data[:,:-1]\n",
        "        y = data[:,-1]\n",
        "        losses = []\n",
        "        errors = []\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            # Gradient Descent\n",
        "            # WRITE CODE HERE\n",
        "\n",
        "            # Update losses\n",
        "            # WRITE CODE HERE]\n",
        "\n",
        "            # Update errors\n",
        "            # WRITE CODE HERE\n",
        "\n",
        "            # Plot\n",
        "            if plot and i % 2 == 0:\n",
        "              clear_output(wait=True)\n",
        "              plt.figure()\n",
        "              scatter(trainset, marker='o')\n",
        "              scatter(testset, marker='x')\n",
        "              decision_boundary(self.w)\n",
        "              finalize_plot(i)\n",
        "              plt.show()\n",
        "\n",
        "        print(\"Training completed: the train error is {:.2f}%\".format(errors[-1]*100))\n",
        "        return np.array(losses), np.array(errors)\n",
        "\n",
        "def test_model(modelclass, w0=[-3.0, 3.0, 0.1], reg=.1, stepsize=.2, plot=False):\n",
        "    \"\"\"Crée une instance de modelclass, entraîne la, calcule le taux d'erreurs sur un\n",
        "    test set, trace les  courbes d'apprentissage et la frontieres de decision.\n",
        "    \"\"\"\n",
        "    model = modelclass(w0, reg)\n",
        "    training_loss, training_error = model.train(trainset, stepsize, 100, plot=plot)\n",
        "    print(\"The test error is {:.2f}%\".format(\n",
        "      model.error_rate(testset[:,:-1], testset[:,-1])*100))\n",
        "    print('Initial weights: ', w0)\n",
        "    print('Final weights: ', model.w)\n",
        "\n",
        "    # learning curves\n",
        "    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8,2))\n",
        "    ax0.plot(training_loss)\n",
        "    ax0.set_title('loss')\n",
        "    ax1.plot(training_error)\n",
        "    ax1.set_title('error rate')\n",
        "\n",
        "    # data plot\n",
        "    plt.figure()\n",
        "    scatter(trainset, marker='x')\n",
        "    scatter(testset, marker='^')\n",
        "    decision_boundary(model.w)\n",
        "    finalize_plot(modelclass.__name__)\n",
        "\n",
        "test_model(LinearModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gElhiGwoiFIc"
      },
      "source": [
        "## 4. Linear Regression/Régression linéaire\n",
        "\n",
        "**Exercice:** Complétez les méthodes `loss` et `gradient`.\n",
        "\n",
        "Rappel la fonction de perte (loss) est donnée par  $$L(w) = \\frac{1}{n}\\sum_{i=1}^n g(w; x_i, y_i) + \\frac{\\lambda}{2} \\| w\\|^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqbnWwUAgaKE"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "\n",
        "class LinearRegression(LinearModel):\n",
        "\n",
        "    def __init__(self, w0, reg):\n",
        "        super().__init__(w0, reg)\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"Calcule la perte moyenne pour une batch X.\n",
        "        Prend en entrée une matrice X et le vecteur y et retourne un scalaire.\n",
        "        \"\"\"\n",
        "        # écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        \"\"\"Calcule le gradient de la fonction de perte par rapport à w pour un batch X.\n",
        "        Prend en entrée une matrice X et le vecteur y.\n",
        "        Retourne un vecteur de la meme taille que w.\n",
        "        \"\"\"\n",
        "        # écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "test_model(LinearRegression, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdR1rrDagj0k"
      },
      "source": [
        "## 5. Perceptron\n",
        "\n",
        "**Exercice:** Complétez les méthodes `loss` et `gradient`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQDgzsXzgfqp"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "\n",
        "class Perceptron(LinearModel):\n",
        "\n",
        "    def __init__(self, w0, reg):\n",
        "        super().__init__(w0, reg)\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        # write code here/écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        # write code here/écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "\n",
        "test_model(Perceptron, reg=0, stepsize=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5vh5zoog0dp"
      },
      "source": [
        "## 6. SVM\n",
        "\n",
        "**Exercice:** Complétez les méthodes `loss` et `gradient`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnLNVq0og0B4"
      },
      "outputs": [],
      "source": [
        "# Question 4\n",
        "\n",
        "class SVM(LinearModel):\n",
        "\n",
        "    def __init__(self, w0, reg):\n",
        "        super().__init__(w0, reg)\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        # write code here/écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        # write code here/écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "\n",
        "test_model(SVM, reg=.1, stepsize=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJz-tBzyqK0_"
      },
      "source": [
        "## 7. Logistic Regression/Régression logistique\n",
        "\n",
        "**Exercice:** Complétez les méthodes `loss` et `gradient`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnoSgkSbhV6f"
      },
      "outputs": [],
      "source": [
        "# Question 5\n",
        "\n",
        "class LogisticRegression(LinearModel):\n",
        "\n",
        "    def __init__(self, w0, reg):\n",
        "        super().__init__(w0, reg)\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        # write code here/écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        # write code here/écrivez votre code ici\n",
        "        pass\n",
        "\n",
        "\n",
        "test_model(LogisticRegression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o85c8vHtosm"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "C'est terminé! Vous avez maintenant réussi à entraîner 4 modèles à l'aide de fonctions de pertes les plus connues. Pour être exacte, la fonction de perte du perceptron n'est plus vraiment utilisée. Savez-vous pourquoi? Êtes-vous en mesure de comparer le comportement de la perte du SVM versus celle du perceptron lorsque vous jouez avec différents hyperparamètres et initialisations? Que pouvez-vous dire du comportement de la frontière de décision du perceptron?\n",
        "\n",
        "Actuellement, les jeux de données en apprentissage machine sont tellement énormes que nous utilisons la descente du gradient stochastique plutôt que la descente du gradient complète. Vous pouvez d'ailleurs essayer de l'implémenter dans la classe parent `LinearModel`.\n",
        "\n",
        "Vous pouvez également explorer différents hyperparamètres et observer vos résultats!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hPBHyUOhf8L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
