{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>Author : Nadine Mohamed (20162200)\n",
    "</br>Date : 12/12/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modify the following\n",
    "path_train_data = './data/train_data.pkl'\n",
    "path_test_data =  './data/test_data.pkl'\n",
    "subset = 50000\n",
    "h_shape = 128\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN:\n",
    "    \"\"\"\n",
    "    A simple fully connected neural network with one hidden layer using ReLU activation\n",
    "    and softmax output for multi-class classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_shape, h_shape, y_shape, lr=0.01, seed=42):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            X_shape : int\n",
    "                The dimensionality of the input features.\n",
    "            h_shape : int\n",
    "                The number of hidden units.\n",
    "            y_shape : int\n",
    "                The number of output classes.\n",
    "            lr : float\n",
    "                Learning rate for gradient descent.\n",
    "            seed : int\n",
    "                Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(X_shape, h_shape) / np.sqrt(X_shape)\n",
    "        self.b1 = np.zeros((1, h_shape))\n",
    "        self.W2 = np.random.randn(h_shape, y_shape) / np.sqrt(h_shape)\n",
    "        self.b2 = np.zeros((1, y_shape))\n",
    "        self.lr = lr\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"\n",
    "        Applies the ReLU activation function element-wise.\n",
    "\n",
    "        args :\n",
    "            x : np.ndarray\n",
    "                Input array.\n",
    "\n",
    "        output : np.ndarray\n",
    "                Result after applying ReLU.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def d_relu(self, x):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the ReLU function.\n",
    "\n",
    "        args\n",
    "        x : np.ndarray\n",
    "            Input array (pre-activation).\n",
    "\n",
    "        output : np.ndarray\n",
    "            Derivative of ReLU.\n",
    "        \"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Applies the softmax function row-wise for classification outputs.\n",
    "\n",
    "        args\n",
    "            x : np.ndarray\n",
    "                Logits before softmax.\n",
    "\n",
    "        output :\n",
    "            n.ndarray\n",
    "                Probabilities after softmax.\n",
    "        \"\"\"\n",
    "        shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_shifted = np.exp(shifted)\n",
    "        return exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy(self, p, y_c):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss.\n",
    "\n",
    "        args :\n",
    "            p : np.ndarray\n",
    "                Predicted probabilities.\n",
    "            y_c : np.ndarray\n",
    "                One-hot encoded true labels.\n",
    "\n",
    "        output :\n",
    "            float\n",
    "                The cross-entropy loss.\n",
    "        \"\"\"\n",
    "        p_capped = np.clip(p, 1e-8, 1 - 1e-8)\n",
    "        N = y_c.shape[0]\n",
    "        loss = -np.sum(y_c * np.log(p_capped)) / N\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, X, Y_onehot):\n",
    "        \"\"\"\n",
    "            Evaluates the model on given data.\n",
    "            \n",
    "            args :\n",
    "            X : np.ndarray\n",
    "                Input data.\n",
    "            Y_onehot : np.ndarray\n",
    "                One-hot encoded true labels.\n",
    "            \n",
    "            ouput :\n",
    "                tuple\n",
    "                    (loss, accuracy)\n",
    "            \"\"\"\n",
    "        p, _ = self.fprop(X)\n",
    "        loss = self.cross_entropy(p, Y_onehot)\n",
    "        preds = np.argmax(p, axis=1)\n",
    "        y_test = np.argmax(Y_onehot, axis=1)\n",
    "        accuracy = np.mean(preds == y_test)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def fprop(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass.\n",
    "\n",
    "        args :\n",
    "            X : np.ndarray\n",
    "                Input data.\n",
    "\n",
    "        output :\n",
    "            tuple\n",
    "                p: predicted probabilities\n",
    "                cache: intermediate values needed for backward pass\n",
    "        \"\"\"\n",
    "\n",
    "        h1 = X.dot(self.W1) + self.b1\n",
    "        a1 = self.relu(h1)\n",
    "        h2 = a1.dot(self.W2) + self.b2\n",
    "        p = self.softmax(h2)\n",
    "        cache = (X, a1, h1, h2, p)\n",
    "        return p, cache\n",
    "\n",
    "    def bprop(self, cache, y_c):\n",
    "        \"\"\"\n",
    "        Performs the backward pass and updates the parameters.\n",
    "\n",
    "        args :\n",
    "            cache : tuple\n",
    "                Values stored during the forward pass.\n",
    "            y_c : np.ndarray\n",
    "                One-hot encoded true labels.\n",
    "        \"\"\"\n",
    "        X, h1, a1, h2, p = cache\n",
    "        N = X.shape[0]\n",
    "\n",
    "        d_h2 = (p - y_c) / N\n",
    "        d_W2 = a1.T.dot(d_h2)\n",
    "        d_b2 = np.sum(d_h2, axis=0, keepdims=True)\n",
    "\n",
    "        d_a1 = d_h2.dot(self.W2.T)\n",
    "        d_h1 = d_a1 * self.d_relu(h1)\n",
    "        d_W1 = X.T.dot(d_h1)\n",
    "        d_b1 = np.sum(d_h1, axis=0, keepdims=True)\n",
    "\n",
    "        # Parameter update\n",
    "        self.W2 -= self.lr * d_W2\n",
    "        self.b2 -= self.lr * d_b2\n",
    "        self.W1 -= self.lr * d_W1\n",
    "        self.b1 -= self.lr * d_b1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for input data.\n",
    "\n",
    "        args :\n",
    "            X : np.ndarray\n",
    "                Input data.\n",
    "\n",
    "        output :\n",
    "            np.ndarray\n",
    "                Predicted class labels.\n",
    "        \"\"\"\n",
    "        p, _ = self.fprop(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "\n",
    "    def fit(\n",
    "        self, X_train, Y_train_onehot, X_val, Y_val_onehot, epochs=50, batch_size=256\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the neural network using mini-batch gradient descent.\n",
    "\n",
    "        args :\n",
    "            X_train : np.ndarray\n",
    "                Training input data.\n",
    "            Y_train_onehot : np.ndarray\n",
    "                One-hot encoded training labels.\n",
    "            X_val : np.ndarray\n",
    "                Validation input data.\n",
    "            Y_val_onehot : np.ndarray\n",
    "                One-hot encoded validation labels.\n",
    "            epochs : int\n",
    "                Number of training epochs.\n",
    "            batch_size : int\n",
    "                Size of each mini-batch.\n",
    "        \"\"\"\n",
    "\n",
    "        # Training with mini-batch gradient descent\n",
    "        num_samples = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.permutation(num_samples)\n",
    "            X_train_shuffled = X_train[idx]\n",
    "            Y_train_onehot_shuffled = Y_train_onehot[idx]\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                X_batch = X_train_shuffled[i : i + batch_size]\n",
    "                Y_batch = Y_train_onehot_shuffled[i : i + batch_size]\n",
    "\n",
    "                p, cache = self.fprop(X_batch)\n",
    "                self.bprop(cache, Y_batch)\n",
    "\n",
    "            train_loss, train_acc = self.evaluate(X_train, Y_train_onehot)\n",
    "            val_loss, val_acc = self.evaluate(X_val, Y_val_onehot)\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{epochs} \"\n",
    "                    f\"- Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                    f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "                )\n",
    "\n",
    "\n",
    "def one_hot_encode(y, n_class):\n",
    "    \"\"\"\n",
    "    One-hot encodes integer labels.\n",
    "    \n",
    "    args :\n",
    "        y : np.ndarray\n",
    "            Array of integer labels.\n",
    "        num_classes : int\n",
    "            Number of classes.\n",
    "    \n",
    "    output :\n",
    "        np.ndarray\n",
    "            One-hot encoded labels.\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(y), n_class))\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 1.0571, Train Acc: 0.5709, Val Loss: 1.0657, Val Acc: 0.5723\n",
      "Epoch 5/100 - Train Loss: 0.9307, Train Acc: 0.6475, Val Loss: 0.9465, Val Acc: 0.6383\n",
      "Epoch 10/100 - Train Loss: 0.8729, Train Acc: 0.6759, Val Loss: 0.8954, Val Acc: 0.6658\n",
      "Epoch 15/100 - Train Loss: 0.8343, Train Acc: 0.6939, Val Loss: 0.8628, Val Acc: 0.6843\n",
      "Epoch 20/100 - Train Loss: 0.8055, Train Acc: 0.7065, Val Loss: 0.8393, Val Acc: 0.6960\n",
      "Epoch 25/100 - Train Loss: 0.7786, Train Acc: 0.7179, Val Loss: 0.8180, Val Acc: 0.7027\n",
      "Epoch 30/100 - Train Loss: 0.7558, Train Acc: 0.7290, Val Loss: 0.7984, Val Acc: 0.7147\n",
      "Epoch 35/100 - Train Loss: 0.7379, Train Acc: 0.7360, Val Loss: 0.7851, Val Acc: 0.7203\n",
      "Epoch 40/100 - Train Loss: 0.7200, Train Acc: 0.7433, Val Loss: 0.7711, Val Acc: 0.7254\n",
      "Epoch 45/100 - Train Loss: 0.7048, Train Acc: 0.7486, Val Loss: 0.7592, Val Acc: 0.7285\n",
      "Epoch 50/100 - Train Loss: 0.6907, Train Acc: 0.7550, Val Loss: 0.7489, Val Acc: 0.7373\n",
      "Epoch 55/100 - Train Loss: 0.6798, Train Acc: 0.7610, Val Loss: 0.7416, Val Acc: 0.7393\n",
      "Epoch 60/100 - Train Loss: 0.6672, Train Acc: 0.7641, Val Loss: 0.7337, Val Acc: 0.7391\n",
      "Epoch 65/100 - Train Loss: 0.6568, Train Acc: 0.7699, Val Loss: 0.7281, Val Acc: 0.7433\n",
      "Epoch 70/100 - Train Loss: 0.6522, Train Acc: 0.7698, Val Loss: 0.7264, Val Acc: 0.7401\n",
      "Epoch 75/100 - Train Loss: 0.6387, Train Acc: 0.7769, Val Loss: 0.7145, Val Acc: 0.7497\n",
      "Epoch 80/100 - Train Loss: 0.6327, Train Acc: 0.7794, Val Loss: 0.7114, Val Acc: 0.7522\n",
      "Epoch 85/100 - Train Loss: 0.6243, Train Acc: 0.7823, Val Loss: 0.7070, Val Acc: 0.7492\n",
      "Epoch 90/100 - Train Loss: 0.6167, Train Acc: 0.7852, Val Loss: 0.7016, Val Acc: 0.7560\n",
      "Epoch 95/100 - Train Loss: 0.6134, Train Acc: 0.7855, Val Loss: 0.7028, Val Acc: 0.7507\n",
      "Epoch 100/100 - Train Loss: 0.6016, Train Acc: 0.7920, Val Loss: 0.6933, Val Acc: 0.7591\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "with open(path_train_data, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "X = np.array(train_data['images'])\n",
    "Y = np.array(train_data['labels'])\n",
    "\n",
    "X = X.reshape(X.shape[0], -1).astype(float)\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0) + 1e-8\n",
    "X_normalized = (X - X_mean) / X_std\n",
    "\n",
    "\n",
    "indices = np.random.permutation(len(X_normalized))[:subset]\n",
    "X_subset = X_normalized[indices]\n",
    "Y_subset = Y[indices]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_subset, Y_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = 4\n",
    "Y_train_onehot = one_hot_encode(Y_train, num_classes)\n",
    "Y_val_onehot = one_hot_encode(Y_val, num_classes)\n",
    "\n",
    "X_shape = X_train.shape[1]\n",
    "y_shape = num_classes\n",
    "\n",
    "model = simpleNN(X_shape, h_shape, y_shape, lr, seed=42)\n",
    "model.fit(X_train, Y_train_onehot, X_val, Y_val_onehot, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "with open('./data/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "X_test_raw = np.array(test_data['images'])\n",
    "X_test_flat = X_test_raw.reshape(X_test_raw.shape[0], -1).astype(float)\n",
    "X_test_normalized = (X_test_flat - X_mean) / X_std\n",
    "\n",
    "test_preds = model.predict(X_test_normalized)\n",
    "\n",
    "with open('submission.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"ID\", \"Class\"])\n",
    "    for i, pred in enumerate(test_preds, start=1):\n",
    "        writer.writerow([i, pred])\n",
    "\n",
    "print(\"Submission saved to 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources/references:\n",
    "\n",
    "1. IFT6390 Course material \n",
    "2. 'Implementation of neural network from scratch using NumPy' : https://www.geeksforgeeks.org/implementation-of-neural-network-from-scratch-using-numpy/\n",
    "2. 'Creating a Neural Network from Scratch Using Python and NumPy' : https://lumos.blog/creating-a-neural-network-from-scratch-using-python-and-numpy/\n",
    "3. The help of AI tools (Co-pilot, ChatGPT, Gemini) \n",
    "AI tools, including GitHub Co-pilot and ChatGPT, were utilized during the coding process. These tools primarily contributed to generating docstrings, refining code structure, and offering suggestions inline within the IDE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
