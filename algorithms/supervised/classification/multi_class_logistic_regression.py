# %%
import numpy as np
import pandas as pd
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# %%
def generate_data(seed=None):
    """
    [DO NOT CHANGE THIS METHOD]
    Generates synthetic dataset with Gaussian mixtures and saves it as CSV files.

    Input:
    - No input required for this method. It generates synthetic data based on pre-defined parameters.

    Process:
    - Creates a dataset with 3 classes where each class is represented by samples drawn from a multivariate normal distribution.
    - The data is shuffled and split into training (60%), validation (20%), and test (20%) sets.
    - The data and labels for each split are saved as CSV files:
        - 'train_features.csv', 'train_labels.csv'
        - 'val_features.csv', 'val_labels.csv'
        - 'test_features.csv', 'test_labels.csv'

    Output:
    - No direct output is returned by this method. The generated data is saved as CSV files.
    """
    np.random.seed(seed)
    
    # Parameters for Gaussian mixtures
    means = [np.random.rand(10) * 2 - 1 for _ in range(3)]
    covs = [np.eye(10) * 0.5 for _ in range(3)]
    
    # Generate data for each class
    X_class0 = np.random.multivariate_normal(means[0], covs[0], 500)
    X_class1 = np.random.multivariate_normal(means[1], covs[1], 500)
    X_class2 = np.random.multivariate_normal(means[2], covs[2], 500)
    
    # Create labels
    y_class0 = np.zeros(500)
    y_class1 = np.ones(500)
    y_class2 = np.ones(500) * 2
    
    # Concatenate the data
    X = np.vstack([X_class0, X_class1, X_class2])
    y = np.hstack([y_class0, y_class1, y_class2])
    
    # Shuffle the data
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)
    X = X[indices]
    y = y[indices]
    
    # Split the data into training, validation, and test sets
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
    # Save to CSV
    pd.DataFrame(X_train).to_csv('train_features.csv', index=False)
    pd.DataFrame(y_train).to_csv('train_labels.csv', index=False)
    pd.DataFrame(X_val).to_csv('val_features.csv', index=False)
    pd.DataFrame(y_val).to_csv('val_labels.csv', index=False)
    pd.DataFrame(X_test).to_csv('test_features.csv', index=False)
    pd.DataFrame(y_test).to_csv('test_labels.csv', index=False)

# %%
def load_and_preprocess_data():
    """
    [DO NOT CHANGE THIS METHOD]
    Loads data from CSV files and standardizes the features using StandardScaler.

    Input:
    - The method expects CSV files generated by the 'generate_data' method:
        - 'train_features.csv', 'train_labels.csv'
        - 'val_features.csv', 'val_labels.csv'
        - 'test_features.csv', 'test_labels.csv'

    Process:
    - Reads the CSV files containing the training, validation, and test features and labels.
    - Standardizes the features (i.e., scales them to have zero mean and unit variance) using `StandardScaler`.
    - Applies the scaling transformation to the training, validation, and test sets.

    Output:
    - Returns six NumPy arrays:
        1. `X_train`: The standardized training feature set.
        2. `y_train`: The labels for the training set.
        3. `X_val`: The standardized validation feature set.
        4. `y_val`: The labels for the validation set.
        5. `X_test`: The standardized test feature set.
        6. `y_test`: The labels for the test set.
    """
    # Load the data
    X_train = pd.read_csv('train_features.csv').values
    y_train = pd.read_csv('train_labels.csv').values.flatten()
    X_val = pd.read_csv('val_features.csv').values
    y_val = pd.read_csv('val_labels.csv').values.flatten()
    X_test = pd.read_csv('test_features.csv').values
    y_test = pd.read_csv('test_labels.csv').values.flatten()

    # Standardize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    return X_train, y_train, X_val, y_val, X_test, y_test

# %%
X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data()

# %%
def make_one_versus_all_labels(y, num_classes):
    """
    Converts the given labels into a one-vs-all format for multi-class classification.

    Input:
    - y: Array of shape (n_samples,) containing the original class labels.
    - num_classes: Integer representing the total number of classes.

    Process:
    - Creates a binary label array where each row corresponds to one sample, and the columns
        represent one-vs-all labels for each class.

    Output:
    - Returns a label array of shape (n_samples, num_classes) with -1 for non-class columns
        and 1 for the true class column.
    """
    out = np.zeros((y.shape[0], num_classes))
    for c in  np.unique(y): 
        out[:,int(c)] = np.where(y==c, 1, -1)
    return out

# %%
y = make_one_versus_all_labels(y_train, len(np.unique(y_train)))
y

# %%
np.where(y[1]==1)[0]

# %%
for i, (x, y_i) in enumerate(zip(X_train, y)):
    j = 1
    print(i, x, y_i)
    print(y_i)
    print(int(np.where(y_i == 1)[0][0]))
    # print((1 if y_i == j else -1) + 2)
    break

# %%
y_i = [-1,  1, -1]

# %%
y_i = np.array([-1,  -1, 1])
np.where(y_i == 1)[0][0]

# %%
int(np.where(y_i == 1)[0])
y_i[np.where(y_i == 1)[0][0]]

# %%
(1 if 1 == 1 else -1)*2

# %%
def compute_loss(X, y, w, C):
    """
    Computes the logistic loss for multi-class classification.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
    - w: Weight matrix of shape (n_features, n_classes).
    - C: Regularization parameter (float).

    Process:
    - Computes the loss using a logistic regression formulation.
    - Adds L2 regularization term based on the weight matrix.

    Output:
    - Returns the scalar loss value (float).
    """
    # TODO vectorize this computation
    n_classes = y.shape[1]
    n_samples = X.shape[0]

    loss = 0.0

    for x, y_row in zip(X, y):
        y_i = np.where(y_row == 1)[0][0]
        for j in range(n_classes):
            I = 1 if y_i == j else -1
            loss += (max(0, 2 - I * np.dot(x, w[:, j])))**2
    
    # get the average loss
    loss/= n_samples

    # add the regularization term
    loss += (C/2)*np.sum(w**2)
    return loss

# %%
# generate ramdom weights for testing
w = np.random.rand(X_train.shape[1], y.shape[1])

# %%
w[:, 0]

# %%
compute_loss(X=X_train, y=y, w=w, C=0.2)

# %%
def compute_gradient(X, y, w, C):
    """
    Computes the gradient of the logistic loss for multi-class classification.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
    - w: Weight matrix of shape (n_features, n_classes).
    - C: Regularization parameter (float).

    Process:
    - Computes the gradient of the logistic loss with respect to the weights.
    - Adds the gradient of the L2 regularization term.

    Output:
    - Returns the gradient matrix of shape (n_features, n_classes).
    """
    # TODO vectorize this computation
    n_classes = y.shape[1]
    n_samples = X.shape[0]
    n_features = X.shape[1]

    gradient = np.zeros_like(w)

    for x, y_row in zip(X, y):
        y_i = np.where(y_row == 1)[0][0]
        for j in range(n_classes):
            I = 1 if y_i == j else -1
            margin = 2 - I * np.dot(x, w[:, j])

            if margin > 0:
                for k in range(n_features):
                    gradient[k, j] += 2 * margin * x[k]

    # get the average gradient
    gradient /= n_samples

    # add the regularization term
    gradient += C * w
    return gradient

# %%
compute_gradient(X=X_train, y=y, w=w, C=0.2)

# %%
def infer(X, w):
    """
    Predicts the class labels for a given feature matrix.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - w: Weight matrix of shape (n_features, n_classes).

    Process:
    - Computes the predicted class probabilities for each sample.
    - Assigns the class with the highest probability as the predicted class label.

    Output:
    - Returns an array of predicted class labels of shape (n_samples,).
    """
    y_pred = np.zeros(X.shape[0])
    
    for i, x in enumerate(X):
        scores = np.dot(x, w)
        y_pred[i] = np.argmax(scores)
    
    return y_pred

# %%
def compute_accuracy(X, y, w):
    """
    Computes the accuracy of predictions using the given weight matrix.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - y: True label vector of shape (n_samples,).
    - w: Weight matrix of shape (n_features, n_classes).

    Process:
    - Predicts the labels for the given feature matrix using the weights.
    - Compares predicted labels with true labels to compute accuracy.

    Output:
    - Returns the accuracy score (float) as a percentage of correct predictions.
    """
    y_pred = infer(X, w)
    accuracy = np.mean(y_pred == y)*100
    return accuracy

# %%


# %%
def fit(X_train, y_train, X_val, y_val,num_classes, epochs,learning_rate, C, batch_size):
    """
    [DO NOT CHANGE THIS METHOD]
    Fits a multi-class logistic regression model using mini-batch gradient descent.

    Input:
    - X_train: Feature matrix of shape (n_train, n_features) for training.
    - y_train: Label matrix of shape (n_train, n_classes) for training.
    - X_val: Feature matrix of shape (n_val, n_features) for validation.
    - y_val: Label matrix of shape (n_val, n_classes) for validation.
    - num_classes: Integer representing the total number of classes.
    - epochs: Integer specifying the number of training epochs.
    - learning_rate: Float value specifying the learning rate for optimization.
    - C: Regularization parameter (float).
    - batch_size: Integer specifying the mini-batch size for training.

    Process:
    - Initializes the weight matrix with zeros.
    - Iterates over the training data for the specified number of epochs.
    - Splits the training data into mini-batches and performs gradient descent.
    - Computes the training and validation losses at the end of each epoch.

    Output:
    - Returns the weight matrix after training.
    """
    num_features = X_train.shape[1]
    w = np.zeros((num_features, num_classes))
    
    # Dictionaries to store the metrics
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    for epoch in range(epochs):
        indices = np.arange(X_train.shape[0])
        np.random.shuffle(indices)
        
        # Mini-batch training
        for i in range(0, X_train.shape[0], batch_size):
            batch_indices = indices[i:i+batch_size]
            X_batch = X_train[batch_indices]
            y_batch = make_one_versus_all_labels(y_train[batch_indices], num_classes)
            
            # Compute gradients and update weights
            grad = compute_gradient(X_batch, y_batch, w, C)
            w -= learning_rate * grad
        
        # Compute training and validation loss and accuracy
        train_loss = compute_loss(X_train, make_one_versus_all_labels(y_train, num_classes), w, C)
        val_loss = compute_loss(X_val, make_one_versus_all_labels(y_val, num_classes), w, C)
        train_acc = compute_accuracy(X_train, y_train, w)
        val_acc = compute_accuracy(X_val, y_val, w)
    
        # Store the metrics
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}")

# %%
X_train, y_train, X_val, y_val, X_test, y_test

# %%
fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, num_classes=3, epochs=5,learning_rate=1, C=0.2, batch_size=17)

# %% [markdown]
# 

# %%
def compute_accuracy(X, y, w):
    """
    Computes the accuracy of predictions using the given weight matrix.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - y: True label vector of shape (n_samples,).
    - w: Weight matrix of shape (n_features, n_classes).

    Process:
    - Predicts the labels for the given feature matrix using the weights.
    - Compares predicted labels with true labels to compute accuracy.

    Output:
    - Returns the accuracy score (float) as a percentage of correct predictions.
    """
    y_pred = infer(X, w)
    accuracy = np.mean(y_pred == y)*100
    return accuracy

# %%
np.mean(y_pred == y_test)

# %%
compute_accuracy(X=X_test, y=y_test, w=w)

# %% [markdown]
# Retired

# %%
def compute_loss(X, y, w, C):
    """
    Computes the logistic loss for multi-class classification.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
    - w: Weight matrix of shape (n_features, n_classes).
    - C: Regularization parameter (float).

    Process:
    - Computes the loss using a logistic regression formulation.
    - Adds L2 regularization term based on the weight matrix.

    Output:
    - Returns the scalar loss value (float).
    """
    # # Calculate the dot product for each class
    # scores = X @ w

    # # Determine the correct class indices
    # labels = np.argmax(y, axis=1)  # Shape: (n_samples,)

    # # Compute the indicator matrix for correct class assignments
    # I = np.where(np.arange(scores.shape[1]) == labels[:, None], 1, -1)

    # # Calculate the loss using max(0, I * scores) as per original calculation
    # losses = np.maximum(0, I * scores)
    # total_loss = np.sum(losses) / X.shape[0]
    
    # # Add the regularization term (sum of weights, not squared)
    # reg_loss = C * np.sum(w)
    # total_loss += reg_loss
    # return total_loss

# %%
def compute_loss(self, X, y, w, C):
    """
    Computes the logistic loss for multi-class classification.

    Input:
    - X: Feature matrix of shape (n_samples, n_features).
    - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.
    - w: Weight matrix of shape (n_features, n_classes).
    - C: Regularization parameter (float).

    Process:
    - Computes the loss using a logistic regression formulation.
    - Adds L2 regularization term based on the weight matrix.

    Output:
    - Returns the scalar loss value (float).
    """
    n_samples = X.shape[0]

    # Compute the scores
    scores = X @ w  # Shape: (n_samples, n_classes)
    
    # Compute the margins
    margins = 2 - scores * y  # Element-wise multiplication
    
    # Apply the hinge function and square
    losses = np.maximum(0, margins) ** 2  # Shape: (n_samples, n_classes)
    
    # Compute the data loss
    data_loss = np.sum(losses) / n_samples
    
    # Compute the regularization loss
    reg_loss = (C / 2) * np.sum(w * w)
    
    # Total loss
    total_loss = data_loss + reg_loss
    
    return total_loss

# %%
import numpy as np
import matplotlib.pyplot as plt

# Import the PracticalHomework2 class from the solution module
from solution import PracticalHomework2


# %%
# Create an instance of PracticalHomework2
practical_hw2 = PracticalHomework2()

# Generate the synthetic dataset (if not already generated)
practical_hw2.generate_data(seed=42)

# Load and preprocess the data
X_train, y_train, X_val, y_val, X_test, y_test = practical_hw2.load_and_preprocess_data()

# %%
# Define hyperparameters
num_classes = 3          # Number of classes
epochs = 200             # Number of training epochs
learning_rate = 0.0001   # Learning rate for gradient descent
batch_size = 100         # Mini-batch size
C_values = [1, 5, 10]    # Regularization parameters

# Initialize a dictionary to store the training histories for each value of C
histories = {}


# %%
for C in C_values:
    print(f"\nTraining with C = {C}")
    # Train the model using the training set and evaluate on the test set
    w, history = practical_hw2.fit(
        X_train=X_train,
        y_train=y_train,
        X_val=X_test,   # Use test set for evaluation
        y_val=y_test,
        num_classes=num_classes,
        epochs=epochs,
        learning_rate=learning_rate,
        C=C,
        batch_size=batch_size
    )
    # Store the history for later plotting
    histories[C] = history


# %%
# Define the range of epochs for plotting
epochs_range = range(1, epochs + 1)

# Plot Training Loss
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, histories[C]['train_loss'], label=f'C = {C}')
plt.title('Training Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot Training Accuracy
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, histories[C]['train_acc'], label=f'C = {C}')
plt.title('Training Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Training Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Test Loss
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, histories[C]['val_loss'], label=f'C = {C}')
plt.title('Test Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Test Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot Test Accuracy
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, histories[C]['val_acc'], label=f'C = {C}')
plt.title('Test Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Test Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# %%
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Import the PracticalHomework2 class
from solution import PracticalHomework2  # Replace 'your_module' with the actual module name if necessary

# Create an instance of the class
hw = PracticalHomework2()

# Generate data (if not already generated)
seed = 42  # You can choose any seed value
hw.generate_data(seed)

# Load and preprocess the data
X_train, y_train, X_val, y_val, X_test, y_test = hw.load_and_preprocess_data()

# Number of classes (as per the dataset)
num_classes = 3

# Define parameters
epochs = 200
learning_rate = 0.0001
batch_size = 100
C_values = [1, 5, 10]

# Dictionaries to store history for each C
history_dict = {}

# Loop over different values of C
for C in C_values:
    print(f"\nTraining with C = {C}")
    # Fit the model
    w, history = hw.fit(
        X_train, y_train,
        X_val, y_val,
        num_classes=num_classes,
        epochs=epochs,
        learning_rate=learning_rate,
        C=C,
        batch_size=batch_size
    )
    # Store the history
    history_dict[C] = history

# Plotting
epochs_range = range(1, epochs + 1)

# Plot Training Loss
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, history_dict[C]['train_loss'], label=f'C = {C}')
plt.title('Training Loss vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot Training Accuracy
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, history_dict[C]['train_acc'], label=f'C = {C}')
plt.title('Training Accuracy vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Test Loss
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, history_dict[C]['val_loss'], label=f'C = {C}')
plt.title('Test Loss vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('Test Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot Test Accuracy
plt.figure(figsize=(10, 6))
for C in C_values:
    plt.plot(epochs_range, history_dict[C]['val_acc'], label=f'C = {C}')
plt.title('Test Accuracy vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('Test Accuracy')
plt.legend()
plt.grid(True)
plt.show()