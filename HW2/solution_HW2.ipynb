{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(seed=None):\n",
    "    \"\"\"\n",
    "    [DO NOT CHANGE THIS METHOD]\n",
    "    Generates synthetic dataset with Gaussian mixtures and saves it as CSV files.\n",
    "\n",
    "    Input:\n",
    "    - No input required for this method. It generates synthetic data based on pre-defined parameters.\n",
    "\n",
    "    Process:\n",
    "    - Creates a dataset with 3 classes where each class is represented by samples drawn from a multivariate normal distribution.\n",
    "    - The data is shuffled and split into training (60%), validation (20%), and test (20%) sets.\n",
    "    - The data and labels for each split are saved as CSV files:\n",
    "        - 'train_features.csv', 'train_labels.csv'\n",
    "        - 'val_features.csv', 'val_labels.csv'\n",
    "        - 'test_features.csv', 'test_labels.csv'\n",
    "\n",
    "    Output:\n",
    "    - No direct output is returned by this method. The generated data is saved as CSV files.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Parameters for Gaussian mixtures\n",
    "    means = [np.random.rand(10) * 2 - 1 for _ in range(3)]\n",
    "    covs = [np.eye(10) * 0.5 for _ in range(3)]\n",
    "    \n",
    "    # Generate data for each class\n",
    "    X_class0 = np.random.multivariate_normal(means[0], covs[0], 500)\n",
    "    X_class1 = np.random.multivariate_normal(means[1], covs[1], 500)\n",
    "    X_class2 = np.random.multivariate_normal(means[2], covs[2], 500)\n",
    "    \n",
    "    # Create labels\n",
    "    y_class0 = np.zeros(500)\n",
    "    y_class1 = np.ones(500)\n",
    "    y_class2 = np.ones(500) * 2\n",
    "    \n",
    "    # Concatenate the data\n",
    "    X = np.vstack([X_class0, X_class1, X_class2])\n",
    "    y = np.hstack([y_class0, y_class1, y_class2])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Save to CSV\n",
    "    pd.DataFrame(X_train).to_csv('train_features.csv', index=False)\n",
    "    pd.DataFrame(y_train).to_csv('train_labels.csv', index=False)\n",
    "    pd.DataFrame(X_val).to_csv('val_features.csv', index=False)\n",
    "    pd.DataFrame(y_val).to_csv('val_labels.csv', index=False)\n",
    "    pd.DataFrame(X_test).to_csv('test_features.csv', index=False)\n",
    "    pd.DataFrame(y_test).to_csv('test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    [DO NOT CHANGE THIS METHOD]\n",
    "    Loads data from CSV files and standardizes the features using StandardScaler.\n",
    "\n",
    "    Input:\n",
    "    - The method expects CSV files generated by the 'generate_data' method:\n",
    "        - 'train_features.csv', 'train_labels.csv'\n",
    "        - 'val_features.csv', 'val_labels.csv'\n",
    "        - 'test_features.csv', 'test_labels.csv'\n",
    "\n",
    "    Process:\n",
    "    - Reads the CSV files containing the training, validation, and test features and labels.\n",
    "    - Standardizes the features (i.e., scales them to have zero mean and unit variance) using `StandardScaler`.\n",
    "    - Applies the scaling transformation to the training, validation, and test sets.\n",
    "\n",
    "    Output:\n",
    "    - Returns six NumPy arrays:\n",
    "        1. `X_train`: The standardized training feature set.\n",
    "        2. `y_train`: The labels for the training set.\n",
    "        3. `X_val`: The standardized validation feature set.\n",
    "        4. `y_val`: The labels for the validation set.\n",
    "        5. `X_test`: The standardized test feature set.\n",
    "        6. `y_test`: The labels for the test set.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    X_train = pd.read_csv('train_features.csv').values\n",
    "    y_train = pd.read_csv('train_labels.csv').values.flatten()\n",
    "    X_val = pd.read_csv('val_features.csv').values\n",
    "    y_val = pd.read_csv('val_labels.csv').values.flatten()\n",
    "    X_test = pd.read_csv('test_features.csv').values\n",
    "    y_test = pd.read_csv('test_labels.csv').values.flatten()\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_versus_all_labels(y, num_classes):\n",
    "    \"\"\"\n",
    "    Converts the given labels into a one-vs-all format for multi-class classification.\n",
    "\n",
    "    Input:\n",
    "    - y: Array of shape (n_samples,) containing the original class labels.\n",
    "    - num_classes: Integer representing the total number of classes.\n",
    "\n",
    "    Process:\n",
    "    - Creates a binary label array where each row corresponds to one sample, and the columns\n",
    "        represent one-vs-all labels for each class.\n",
    "\n",
    "    Output:\n",
    "    - Returns a label array of shape (n_samples, num_classes) with -1 for non-class columns\n",
    "        and 1 for the true class column.\n",
    "    \"\"\"\n",
    "    out = np.zeros((y.shape[0], num_classes))\n",
    "    for c in  np.unique(y): \n",
    "        out[:,int(c)] = np.where(y==c, 1, -1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1., -1.],\n",
       "       [ 1., -1., -1.],\n",
       "       [ 1., -1., -1.],\n",
       "       ...,\n",
       "       [-1., -1.,  1.],\n",
       "       [ 1., -1., -1.],\n",
       "       [ 1., -1., -1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = make_one_versus_all_labels(y_train, len(np.unique(y_train)))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900,)\n",
      "(900,)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "for c in y.T :\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w, C):\n",
    "    \"\"\"\n",
    "    Computes the logistic loss for multi-class classification.\n",
    "\n",
    "    Input:\n",
    "    - X: Feature matrix of shape (n_samples, n_features).\n",
    "    - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.\n",
    "    - w: Weight matrix of shape (n_features, n_classes).\n",
    "    - C: Regularization parameter (float).\n",
    "\n",
    "    Process:\n",
    "    - Computes the loss using a logistic regression formulation.\n",
    "    - Adds L2 regularization term based on the weight matrix.\n",
    "\n",
    "    Output:\n",
    "    - Returns the scalar loss value (float).\n",
    "    \"\"\"\n",
    "    n_classes = y.shape[1]\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        loss = np.zeros_like(w)\n",
    "        for j in range(n_classes):\n",
    "            a = 2 - y[i, j]*(np.dot(x, w[:, j]))\n",
    "\n",
    "            if a > 0:\n",
    "                # print(a, y[i, j], x.shape, C, w[:, j].shape)\n",
    "                loss[:, j] += -2 * a * y[i, j] *x + C*w[:, j]\n",
    "        \n",
    "        total_loss += np.sum(loss)\n",
    "\n",
    "    total_loss/= n_samples\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([[1,2,3],[1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3]])\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 30\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(X, y, w, C)\u001b[0m\n\u001b[0;32m     26\u001b[0m         a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m y[i, j]\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39mdot(x, w[:, j]))\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;66;03m# print(a, y[i, j], x.shape, C, w[:, j].shape)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m             \u001b[43mloss\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(loss)\n\u001b[0;32m     34\u001b[0m total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "compute_loss(X=X_train, y=y, w=w, C=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(self, X, y, w, C):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the logistic loss for multi-class classification.\n",
    "\n",
    "    Input:\n",
    "    - X: Feature matrix of shape (n_samples, n_features).\n",
    "    - y: Label matrix of shape (n_samples, n_classes), formatted for one-vs-all classification.\n",
    "    - w: Weight matrix of shape (n_features, n_classes).\n",
    "    - C: Regularization parameter (float).\n",
    "\n",
    "    Process:\n",
    "    - Computes the gradient of the logistic loss with respect to the weights.\n",
    "    - Adds the gradient of the L2 regularization term.\n",
    "\n",
    "    Output:\n",
    "    - Returns the gradient matrix of shape (n_features, n_classes).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(self, X, w):\n",
    "    \"\"\"\n",
    "    Predicts the class labels for a given feature matrix.\n",
    "\n",
    "    Input:\n",
    "    - X: Feature matrix of shape (n_samples, n_features).\n",
    "    - w: Weight matrix of shape (n_features, n_classes).\n",
    "\n",
    "    Process:\n",
    "    - Computes the predicted class probabilities for each sample.\n",
    "    - Assigns the class with the highest probability as the predicted class label.\n",
    "\n",
    "    Output:\n",
    "    - Returns an array of predicted class labels of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self, X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of predictions using the given weight matrix.\n",
    "\n",
    "    Input:\n",
    "    - X: Feature matrix of shape (n_samples, n_features).\n",
    "    - y: True label vector of shape (n_samples,).\n",
    "    - w: Weight matrix of shape (n_features, n_classes).\n",
    "\n",
    "    Process:\n",
    "    - Predicts the labels for the given feature matrix using the weights.\n",
    "    - Compares predicted labels with true labels to compute accuracy.\n",
    "\n",
    "    Output:\n",
    "    - Returns the accuracy score (float) as a percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
