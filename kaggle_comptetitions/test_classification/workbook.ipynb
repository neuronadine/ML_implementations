{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Classification Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is a NumPy array representing term vector counts for training, where each row corresponds to a document and each column represents a term in the vocabulary. \n",
    "    The values (mostly 0s) indicate the count of each term in the respective document, forming a sparse matrix.\n",
    "\"\"\"\n",
    "data_train = np.load(\"data_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A similar NumPy array for testing. You need to create labels for this test set and submit.\n",
    "\"\"\"\n",
    "data_test = np.load(\"data_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains a mapping between the terms (words) and their corresponding indices in the term vector matrix.\n",
    "\"\"\"\n",
    "vocab_map = np.load(\"vocab_map.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains the labels or target values for the training dataset (0 or 1).\n",
    "\"\"\"\n",
    "# Load the labels from the CSV file\n",
    "label_train = pd.read_csv('label_train.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0001', ..., 'zs', 'zsda', 'zsl'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9417</th>\n",
       "      <td>9417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418</th>\n",
       "      <td>9418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>9419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9420</th>\n",
       "      <td>9420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>9421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  label\n",
       "0        0      0\n",
       "1        1      0\n",
       "2        2      0\n",
       "3        3      0\n",
       "4        4      1\n",
       "...    ...    ...\n",
       "9417  9417      0\n",
       "9418  9418      1\n",
       "9419  9419      0\n",
       "9420  9420      0\n",
       "9421  9421      0\n",
       "\n",
       "[9422 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_train, data_test, vocab_map, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a data frame with all the features \n",
    "# Visulize the data .. what type of distribution is this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26354,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9422, 26354)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 0.6865464351606382\n",
      "Iteration 100: Cost 0.5567154342318743\n",
      "Iteration 200: Cost 0.5546245399374068\n",
      "Iteration 300: Cost 0.5542287125959212\n",
      "Iteration 400: Cost 0.5538698230913401\n",
      "Iteration 500: Cost 0.5535128134175942\n",
      "Iteration 600: Cost 0.5531568778095659\n",
      "Iteration 700: Cost 0.5528019932218066\n",
      "Iteration 800: Cost 0.5524481555477394\n",
      "Iteration 900: Cost 0.5520953612762602\n",
      "Macro F1 Score on Validation Set: 0.43034146835737835\n",
      "Iteration 0: Cost 0.6865548714503225\n",
      "Iteration 100: Cost 0.5569029955651373\n",
      "Iteration 200: Cost 0.5548225588677439\n",
      "Iteration 300: Cost 0.5544332181284596\n",
      "Iteration 400: Cost 0.5540806492700086\n",
      "Iteration 500: Cost 0.5537299302272252\n",
      "Iteration 600: Cost 0.5533802598146694\n",
      "Iteration 700: Cost 0.553031615246713\n",
      "Iteration 800: Cost 0.5526839925425006\n",
      "Iteration 900: Cost 0.5523373883116114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Data\n",
    "\n",
    "# Load the training data\n",
    "X_train = np.load('data_train.npy')\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv('label_train.csv', index_col=False)\n",
    "\n",
    "# Ensure the labels are sorted by 'ID' to match X_train\n",
    "labels_df = labels_df.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "# Extract the 'label' column as a NumPy array\n",
    "y_train = labels_df['label'].values\n",
    "\n",
    "# Load the test data\n",
    "X_test = np.load('data_test.npy')\n",
    "\n",
    "# Step 2: Preprocess the Data\n",
    "\n",
    "# Compute TF\n",
    "tf_train = X_train / np.sum(X_train, axis=1, keepdims=True)\n",
    "tf_test = X_test / np.sum(X_test, axis=1, keepdims=True)\n",
    "\n",
    "# Replace any NaN values resulting from division by zero\n",
    "tf_train = np.nan_to_num(tf_train)\n",
    "tf_test = np.nan_to_num(tf_test)\n",
    "\n",
    "# Compute IDF\n",
    "df = np.count_nonzero(X_train, axis=0)\n",
    "N = X_train.shape[0]\n",
    "idf = np.log((N + 1) / (df + 1)) + 1\n",
    "\n",
    "# Compute TF-IDF\n",
    "X_train_tfidf = tf_train * idf\n",
    "X_test_tfidf = tf_test * idf\n",
    "\n",
    "# Select Top K Features\n",
    "tfidf_sums = np.sum(X_train_tfidf, axis=0)\n",
    "k = 5000  # Adjust based on memory constraints\n",
    "top_k_indices = np.argsort(tfidf_sums)[-k:]\n",
    "X_train_reduced = X_train_tfidf[:, top_k_indices]\n",
    "X_test_reduced = X_test_tfidf[:, top_k_indices]\n",
    "\n",
    "# Add Intercept Term\n",
    "X_train_aug = np.hstack((np.ones((X_train_reduced.shape[0], 1)), X_train_reduced))\n",
    "X_test_aug = np.hstack((np.ones((X_test_reduced.shape[0], 1)), X_test_reduced))\n",
    "\n",
    "# Step 3: Split the Data into Training and Validation Sets\n",
    "np.random.seed(42)\n",
    "indices = np.arange(X_train_aug.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * len(indices))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "X_train_fold = X_train_aug[train_indices]\n",
    "y_train_fold = y_train[train_indices]\n",
    "X_val_fold = X_train_aug[val_indices]\n",
    "y_val_fold = y_train[val_indices]\n",
    "\n",
    "# Step 4: Implement Logistic Regression Functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, theta, lambda_):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    epsilon = 1e-5\n",
    "    cost = (-1/m) * (y @ np.log(h + epsilon) + (1 - y) @ np.log(1 - h + epsilon))\n",
    "    reg_cost = cost + (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))\n",
    "    return reg_cost\n",
    "\n",
    "def compute_gradient(X, y, theta, lambda_):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    error = h - y\n",
    "    gradient = (1/m) * (X.T @ error)\n",
    "    gradient[1:] += (lambda_ / m) * theta[1:]\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iters, lambda_):\n",
    "    cost_history = []\n",
    "    for i in range(num_iters):\n",
    "        gradient = compute_gradient(X, y, theta, lambda_)\n",
    "        theta -= alpha * gradient\n",
    "        cost = compute_cost(X, y, theta, lambda_)\n",
    "        cost_history.append(cost)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Iteration {i}: Cost {cost}')\n",
    "    return theta, cost_history\n",
    "\n",
    "# Step 5: Train the Model on Training Fold\n",
    "theta = np.zeros(X_train_fold.shape[1])\n",
    "alpha = 0.1\n",
    "num_iters = 1000\n",
    "lambda_ = 0.1\n",
    "theta, cost_history = gradient_descent(X_train_fold, y_train_fold, theta, alpha, num_iters, lambda_)\n",
    "\n",
    "# Step 6: Validate the Model\n",
    "probabilities_val = sigmoid(X_val_fold @ theta)\n",
    "y_val_pred = (probabilities_val >= 0.5).astype(int)\n",
    "\n",
    "def compute_macro_f1(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    f1_scores = []\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        fp = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        fn = np.sum((y_pred != cls) & (y_true == cls))\n",
    "        precision = tp / (tp + fp + 1e-7)\n",
    "        recall = tp / (tp + fn + 1e-7)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "        f1_scores.append(f1)\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    return macro_f1\n",
    "\n",
    "macro_f1 = compute_macro_f1(y_val_fold, y_val_pred)\n",
    "print(f'Macro F1 Score on Validation Set: {macro_f1}')\n",
    "\n",
    "# Step 7: Retrain on Full Training Data\n",
    "theta = np.zeros(X_train_aug.shape[1])\n",
    "theta, cost_history = gradient_descent(X_train_aug, y_train, theta, alpha, num_iters, lambda_)\n",
    "\n",
    "# Step 8: Make Predictions on Test Data\n",
    "probabilities = sigmoid(X_test_aug @ theta)\n",
    "y_pred = (probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Step 9: Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'ID': np.arange(len(y_pred)),\n",
    "    'label': y_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score on Validation Set (Naive Bayes): 0.7122338438977459\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Data\n",
    "X_train = np.load('data_train.npy')\n",
    "labels_df = pd.read_csv('label_train.csv', index_col=False)\n",
    "labels_df = labels_df.sort_values('ID').reset_index(drop=True)\n",
    "y_train = labels_df['label'].values\n",
    "X_test = np.load('data_test.npy')\n",
    "\n",
    "# Implement Multinomial Naive Bayes\n",
    "\n",
    "# Step 1: Calculate Class Priors\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "class_priors = class_counts / y_train.shape[0]\n",
    "\n",
    "# Step 2: Calculate Conditional Probabilities with Laplace Smoothing\n",
    "num_classes = len(unique_classes)\n",
    "num_features = X_train.shape[1]\n",
    "alpha = 1  # Laplace smoothing parameter\n",
    "\n",
    "class_term_counts = np.zeros((num_classes, num_features))\n",
    "class_total_counts = np.zeros(num_classes)\n",
    "\n",
    "for idx, cls in enumerate(unique_classes):\n",
    "    X_cls = X_train[y_train == cls]\n",
    "    class_term_counts[idx, :] = np.sum(X_cls, axis=0)\n",
    "    class_total_counts[idx] = np.sum(class_term_counts[idx, :])\n",
    "\n",
    "conditional_probs = (class_term_counts + alpha) / (class_total_counts[:, None] + alpha * num_features)\n",
    "\n",
    "# Step 3: Convert to Log Probabilities\n",
    "log_class_priors = np.log(class_priors)\n",
    "log_conditional_probs = np.log(conditional_probs)\n",
    "\n",
    "# Step 4: Define Predict Function\n",
    "def predict_multinomial_nb(X):\n",
    "    log_probs = np.zeros((X.shape[0], num_classes))\n",
    "    for idx in range(num_classes):\n",
    "        log_prob = X @ log_conditional_probs[idx, :].T\n",
    "        log_probs[:, idx] = log_class_priors[idx] + log_prob\n",
    "    return unique_classes[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "# Step 5: Evaluate on Validation Set\n",
    "# Split data into training and validation sets\n",
    "np.random.seed(42)\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * len(indices))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "X_train_fold = X_train[train_indices]\n",
    "y_train_fold = y_train[train_indices]\n",
    "X_val_fold = X_train[val_indices]\n",
    "y_val_fold = y_train[val_indices]\n",
    "\n",
    "# Recalculate counts on training fold\n",
    "class_term_counts = np.zeros((num_classes, num_features))\n",
    "class_total_counts = np.zeros(num_classes)\n",
    "\n",
    "for idx, cls in enumerate(unique_classes):\n",
    "    X_cls = X_train_fold[y_train_fold == cls]\n",
    "    class_term_counts[idx, :] = np.sum(X_cls, axis=0)\n",
    "    class_total_counts[idx] = np.sum(class_term_counts[idx, :])\n",
    "\n",
    "conditional_probs = (class_term_counts + alpha) / (class_total_counts[:, None] + alpha * num_features)\n",
    "log_class_priors = np.log(class_counts / y_train_fold.shape[0])\n",
    "log_conditional_probs = np.log(conditional_probs)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = predict_multinomial_nb(X_val_fold)\n",
    "\n",
    "# Define Macro F1 Score Function\n",
    "def compute_macro_f1(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    f1_scores = []\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        fp = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        fn = np.sum((y_pred != cls) & (y_true == cls))\n",
    "        precision = tp / (tp + fp + 1e-7)\n",
    "        recall = tp / (tp + fn + 1e-7)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "        f1_scores.append(f1)\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    return macro_f1\n",
    "\n",
    "macro_f1 = compute_macro_f1(y_val_fold, y_val_pred)\n",
    "print(f'Macro F1 Score on Validation Set (Naive Bayes): {macro_f1}')\n",
    "\n",
    "# Step 6: Retrain on Full Training Data and Predict on Test Set\n",
    "# Recalculate counts using full training data\n",
    "class_term_counts = np.zeros((num_classes, num_features))\n",
    "class_total_counts = np.zeros(num_classes)\n",
    "\n",
    "for idx, cls in enumerate(unique_classes):\n",
    "    X_cls = X_train[y_train == cls]\n",
    "    class_term_counts[idx, :] = np.sum(X_cls, axis=0)\n",
    "    class_total_counts[idx] = np.sum(class_term_counts[idx, :])\n",
    "\n",
    "conditional_probs = (class_term_counts + alpha) / (class_total_counts[:, None] + alpha * num_features)\n",
    "log_class_priors = np.log(class_priors)\n",
    "log_conditional_probs = np.log(conditional_probs)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = predict_multinomial_nb(X_test)\n",
    "\n",
    "# Step 7: Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'ID': np.arange(len(y_pred)),\n",
    "    'label': y_pred\n",
    "})\n",
    "submission.to_csv('submission_nb.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "from paddleocr import PaddleOCR\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/21 01:14:49] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\ns99a\\\\OneDrive - Universite de Montreal\\\\P09_AI Msc\\\\A24\\\\IFT6390\\\\ML_implementations\\\\venv\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "ocr = PaddleOCR(lang=\"en\",\n",
    "                # det_model_dir = \"C:/Users/ns99a/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/\",\n",
    "                # rec_model_dir = \"C:/Users/ns99a/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/\",\n",
    "                # cls_model_dir = \"C:/Users/ns99a/.paddleocr/whl/cls/en/ch_ppocr_mobile_v2.0_cls_infer/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/21 01:03:43] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\ns99a\\\\OneDrive - Universite de Montreal\\\\P09_AI Msc\\\\A24\\\\IFT6390\\\\ML_implementations\\\\venv\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "ocr = PaddleOCR(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/latin_PP-OCRv3_rec_infer.tar to C:\\Users\\ns99a/.paddleocr/whl\\rec\\latin\\latin_PP-OCRv3_rec_infer\\latin_PP-OCRv3_rec_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9930/9930 [00:34<00:00, 288.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/21 01:04:30] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\rec\\\\latin\\\\latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\ns99a\\\\OneDrive - Universite de Montreal\\\\P09_AI Msc\\\\A24\\\\IFT6390\\\\ML_implementations\\\\venv\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='C:\\\\Users\\\\ns99a/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='fr', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ocr = PaddleOCR(lang=\"fr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
